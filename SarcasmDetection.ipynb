{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5b1d24-df63-4ed4-be61-f7d263d1ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anas\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anas\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 3.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/10.5 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.4/10.5 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.9/10.5 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.2/10.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.3/10.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.6/10.5 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.1/10.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.9/10.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.1/10.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.4/10.5 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.7/10.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.5 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 933.6 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.32.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ffdc67-890b-4c35-9dd6-1722fc14c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcc96a7-6a89-4d5e-b867-6d337e4f65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataset/Sarcasm_Headlines_Dataset.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7402b4b4-9297-4109-ad67-f9b5ef8c28c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26709, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51bb0831-c7ce-4b53-924b-fa8ead43b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f5bdc9-e846-41d4-9aa7-b35ade882479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0aeda2-638e-45c2-a96d-8ab7c225f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191279d0-65fd-4ddf-b7b1-736635db1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['article_link'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5bc578-041a-4960-92c4-22aaf65825df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1055f9a0-3bac-4e09-9559-ea2fbd519706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       "       \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "       \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       "       ..., 'reparations and obama',\n",
       "       'israeli ban targeting boycott supporters raises alarm abroad',\n",
       "       'gourmet gifts for the foodie 2014'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc82599-5208-45b9-a8cd-283dffeb2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df['headline'])\n",
    "y= np.array(df['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af331ed3-51e5-44be-a3ae-df2c34c10561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_test,y_test,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d8c4344-1b2c-4912-9ce5-e2d21ccaa417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18695,), (4007,), (4006,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e9cdfc7-d737-4a0c-92bd-eb1e1d2e0923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18695,), (4007,), (4006,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf1f5de-18a3-4c4e-9d7b-a6945b89437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8087d261fc447cb790a3958a29adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c46ea58c5b445df8de40f0bbbec7e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62de1d88cf8044ba9bb85d576f759500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ea4307601e4b8ea4760fa8ff64139b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb5dbb9888c4ec890f3df9190b4c474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e5510f-4887-4c37-a021-a1ce382249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = [tokenizer(\n",
    "            x,\n",
    "            max_length = 100,\n",
    "            truncation = True,\n",
    "            padding='max_length',\n",
    "            return_tensors = 'pt').to(device)\n",
    "            for x in X            \n",
    "        ]\n",
    "        self.Y = torch.tensor(Y, dtype = torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, indx):\n",
    "        return self.X[indx],self.Y[indx]\n",
    "\n",
    "training_data = dataset(X_train,y_train)\n",
    "validation_data = dataset(X_val, y_val)\n",
    "testing_data = dataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8dd12c-cf61-4f2a-a7c7-4a6e8d205175",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80efa93b-3d1c-49a6-9a6b-dc80e8f296da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size = BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size = BATCH_SIZE, shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2633828d-2a50-4891-ba96-234e47972999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.linear1 = nn.Linear(768, 384)\n",
    "        self.linear2 = nn.Linear(384,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert(input_ids, attention_mask, return_dict = False)[0][:,0]\n",
    "        output = self.linear1(pooled_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a6573d-eea5-47ec-9d86-9d008e265961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to preserve the pretrained knowledge of the model\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "model = MyModel(bert_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "812048f8-808f-491a-8bce-7db6a56ee621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (linear1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (linear2): Linear(in_features=384, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a562871-52c2-481b-910a-c1ef3b87e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr =LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a384b24-6c61-475e-a2c6-4daba49bec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_train_plot = []\n",
    "total_acc_train_plot = []\n",
    "total_loss_validation_plot = []\n",
    "total_acc_validation_plot = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    for indx, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "\n",
    "        prediction = model(inputs['input_ids'].squeeze(1), inputs['attention_mask'].squeeze(1)).squeeze(1)\n",
    "        batch_loss = criterion(prediction, labels)\n",
    "        total_loss_train += batch_loss.item()\n",
    "\n",
    "        acc = (prediction.round() == labels).sum().item()\n",
    "\n",
    "        total_acc_train +=acc\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for indx, data in enumerate(validation_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            prediction = model(inputs['input_ids'].squeeze(1), inputs['attention_mask'].squeeze(1)).squeeze(1)\n",
    "            batch_loss = criterion(prediction, labels)\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc = (prediction.round() == labels).sum().item()\n",
    "            total_acc_val += acc\n",
    "# we divide the total loss for normalization the data as the data will be big for plotting\n",
    "    total_loss_train_plot.append(round(total_loss_train/1000, 4))\n",
    "    total_loss_validation_plot.append(round(total_loss_val/1000, 4))\n",
    "    total_acc_train_plot.append(round(total_acc_train/training_data.__len__()*100,4))\n",
    "    total_acc_validation_plot.append(round(total_acc_val/validation_data.__len__())*100,4)\n",
    "\n",
    "    print(f\"\"\"\n",
    "             Epoch No. {epoch+1}\n",
    "             Train Loss: {round(total_loss_train/1000, 4)}\n",
    "             Train Accuracy: {round(total_acc_train/training_data.__len__()*100,4)}\n",
    "             Validation Loss: {round(total_loss_val/1000, 4)}\n",
    "             Validation Accuracy: {round(total_acc_val/validation_data.__len__()*100,4)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bfc87-6ac0-4d9c-8801-27e4a1a32360",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss_test = -\n",
    "    total_acc_test = 0\n",
    "\n",
    "    for indx, data in enumerate(validation_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "\n",
    "            prediction = model(inputs['input_ids'].squeeze(1), inputs['attention_mask'].squeeze(1)).squeeze(1)\n",
    "            batch_loss = criterion(prediction, labels)\n",
    "            total_loss_test += batch_loss.item()\n",
    "\n",
    "            acc = (prediction.round() == labels).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "    print(f'Accuracy Score on testing data: {round(total_acc_test/testing_data.__len__()*100,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf9f2c-c154-44d3-840c-49fc65beb895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
